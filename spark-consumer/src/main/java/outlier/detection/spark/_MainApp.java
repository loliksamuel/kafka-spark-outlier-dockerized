package outlier.detection.spark;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.mongodb.BasicDBObject;
import com.mongodb.hadoop.MongoOutputFormat;
import com.mongodb.util.JSON;
import org.apache.hadoop.conf.Configuration;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.streaming.Duration;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaPairDStream;
import org.apache.spark.streaming.api.java.JavaPairReceiverInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.kafka.KafkaUtils;
import org.bson.BSONObject;
import outlier.detection.dto.OutputMessage;
import outlier.detection.spark.input.InputMessage;
import outlier.detection.spark.input.MessageParser;
import outlier.detection.spark.processor.Processor;
import scala.Tuple2;

import java.util.HashMap;
import java.util.Map;

//import org.apache.spark.examples.streaming.StreamingExamples;

/**
 * this spark Consumes stream messages from one or more kafka topics  and does outlier detection.
 *
 * Usage: _MainApp <zkQuorum> <group> <topics> <numThreads>
 *   <zkQuorum>  list of one or more zookeeper servers that make quorum
 *   <group>     name of kafka consumer group
 *   <topics>    list of one or more kafka topics to consume from
 *   <numThreads> number of threads the kafka consumer should use
 *
 * To run this example:
 *   `$ bin/spark-submit outlier.detection.spark._MainApp zoo01,zoo02, \
 *    zoo03 my-consumer-group topic1,topic2 1`
 */

public final class _MainApp {
 
  private static final MessageParser PARSER = new MessageParser();
  private static final Processor  PROCESSOR = new Processor();
  private static final ObjectMapper  MAPPER = new ObjectMapper();
  
  private static final String MONGO_HOST = "mongo_host";
  
  private _MainApp() {
  }

  public static void main(String[] args) {

      System.out.println("before u start this streaming job, u must start zookeeper on port 2181 :   zookeeper-server-start /usr/local/etc/kafka/zookeeper.properties ");
      System.out.println("spark ui uri=localhost:4040");
      String mongoURI = "mongodb://" + MONGO_HOST + ":27017/outliers.output";
      System.out.println("mongo.output.uri="+mongoURI+"   http://localhost:8080/browser/#/  user admin pwd changeit");


      if (args.length < 4) {
        System.err.println("Usage error: missing args <zkQuorum> <kafkaConsumerGroupId> <topics> <numThreads>");
        System.err.println("putting default hard coded params");
        args[0]="localhost:2181";//zookeeperHost
        args[1]= "1";   //kafkaConsumerGroupId
        args[2]= "test";//topicName
        args[3]= "3";   //threads
    }
      String zookeeperHost = args[0];//localhost:2181
      String groupId       = args[1];//groupId
//    StreamingExamples.setStreamingLogLevels();
    SparkConf sparkConf = new SparkConf().setAppName("_MainApp")
                                         //.setMaster("local[*]")
                                         //.setMaster("spark://Impetus-NL163U:7077");
                                            ;





    // Create the stream context with 2 seconds batch interval
    JavaStreamingContext sc = new JavaStreamingContext(sparkConf, new Duration(2000));

    int numThreads = Integer.parseInt(args[3]);
    Map<String, Integer> topicMap = new HashMap();
    String[] topics = args[2].split(",");
    for (String topic: topics) {
      topicMap.put(topic, numThreads);
    }

    JavaPairReceiverInputDStream<String, String> messages = KafkaUtils.createStream(sc, zookeeperHost, groupId, topicMap);

    JavaDStream<OutputMessage> records = messages.map((Function<Tuple2<String, String>, InputMessage>) tuple2 -> PARSER.parse(tuple2._2()))
                                                 .filter((Function<InputMessage, Boolean>) msg -> msg!=null)
                                                 .map((Function<InputMessage, OutputMessage>) input -> PROCESSOR.process(input));


    
    
 // Output contains tuples of (null, BSONObject) - ObjectId will be generated by Mongo driver if null
    JavaPairDStream<Object, BSONObject> resultObj = records.mapToPair((PairFunction<OutputMessage, Object, BSONObject>) outputMessage
                                                                    -> {

                                                                            BasicDBObject obj = null;
                                                                            try {
                                                                                String str = MAPPER.writeValueAsString(outputMessage);
                                                                                obj = (BasicDBObject)JSON.parse(str);
                                                                            } catch (JsonProcessingException e) {
                                                                                e.printStackTrace();
                                                                                return null;
                                                                            }
                                                                            return new Tuple2<>(null, obj);
                                                                        });

      Configuration outputMongoConfig = new Configuration();
      outputMongoConfig.set("mongo.output.uri", mongoURI);

      // Only MongoOutputFormat and config are relevant
   // resultObj.saveAsNewAPIHadoopFiles("file:///bogus", Object.class, Object.class, MongoOutputFormat.class, config);
      resultObj.saveAsNewAPIHadoopFiles("file:///bogus", "suffix", Object.class, BSONObject.class, MongoOutputFormat.class, outputMongoConfig);
      //resultObj.write.mode(SaveMode.Overwrite).parquet( IO_DIRNAME+"df/dfTest2016")//for zeppelin)//6.4mil
      //resultObj.describe().show();
	//StreamingContext.stop(true, true)

      resultObj.print();
      sc.start();
      try {
          sc.awaitTermination();
      } catch (InterruptedException e) {
          e.printStackTrace();
      }
  }
}